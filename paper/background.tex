% !TEX root = ./cvl.tex
\section{Background}
\label{sec:background}

\marcos{We should clearly state what the problem is in this section, in a separate subsection.}

\marcos{Here is where your basic definitions go, e.g., what is a cell, what are zoom levels, how are objects typically selected when creating a map, what are basic and implicit constraints, such as adjacency and zoom consistency, what are application-specific constraints, what is importance/weight, etc.}


\subsection{Conflicts and conflict sets}
\label{sec:conflicts}
\kostas{move to background}
The principle of constant information density implies that we must choose a subset of the records to be visible at each zoom-level of a map in order to satisfy a constraint. Of couser we only have to choose between records whenever a cartographic constraint is actually violated. In our work we use the term \emph{conflict set} to mean a set of records that together are in \emph{conflict}. Records are in conflict when they cause a specific cartographic constraint to be violated at a given zoom-level. Let us consider the proximity constraint introduced before. Using this constraint, there is a conflict for all pairs of records that are less than $d$ pixels apart, and each conflict set consists of one of these pairs. A record can be in several conflict sets if it is too close to more than one record.
%\marcos{This section should explain the core idea of CVL (implicit vs. explicit), and the core idea of each statement in CVL (generalize, create constraint). You may also need a forward pointer to the semantics of the language in the Optimization Models section.}

%\kostas{Move the following to related work once Marcos is done}
%Rule-based languages like Styled Layer Descriptor (SLD)  and Mapnik XML serve a similar purpose as CVL, but using a different approach. The user explicitly decides the filtering of records at each zoom level and how records are presented. CVL is only concerned with the filtering, but is implicit about the exact zoom-level at which a record will appear.

% \marcos{\emph{Condition} in the paragraph above not so clear? }


\subsubsection{Cartographic constraints in CVL}
\label{sec:cartographic-constraints-in-cvl}
\kostas{move to background}
A cartographic constraint in CVL is a condition that must hold for all subsets of a given size. Subsets of records for which the condition does not hold, are said to be in conflict. As part of formulating a constraint, the user writes SQL that finds conflicts for this constraint. Part of the CVL formulation of the proximity constraint is an SQL statement that finds all records that are too near each other at a given zoom-level. The contract between the user and the CVL framework is that the user code must generate $\langle cid, rid \rangle$ tuples that represent the conflict sets. The sematics are that $rid$ is the ID of a record which is a member of a conflict set uniquely identified by $cid$. As an example, the user code for the proximity constraint generates two tuples for each conflict found.

Given the set of conflict tuples generated by the user constraint code, the framework must decide how to resolve the conflicts. A conflict in CVL can always be resolved by removing a subset of the constituent records from the given zoom-level. How many records needs to be removed is also specified in the user code together with the code that finds the conflicts. For the proximity constraint we have to delete one record to resolve each conflict. How this is defined as user code is explained in Section~\ref{sec:create-constraint-statement}.

An example of a cartographic constraint in CVL is the \emph{proximity constraint} which states that at all zoom-levels all visible records must be separated by at least $d$ pixels. CVL does not generally need the user to specify at which zoom-level a record is too close to other records, only what the user considers to be "too close". The formulation of the proximity constraint implies that at each zoom-level a (possibly empty) subset of records must be removed in order to respect the constraint, and this is a general property of constraints in CVL. Which records are prioritized over others is controlled by assigning weights to records to indicate their importance. The implicit goal of evaluating a CVL query is to maintaining as much aggregate weight at each zoom-level as possible while satisfying all constraints.

%\marcos{What about the notion of conflicts and conflict sets? Those are pretty important.}

\subsubsection{Weighting records}
\kostas{move to background}
Records are assigned a weight by user defined code which guides how the CVL framework will resolve conflicts.  The intuitive notion of weight is that it represents the relative importance of a record. In general, the CVL framework will try to find a solution for each zoom-level such that all constraints are  satisfied and such that the aggregate weight of records that are removed is minimized. For example, if two records constitute a conflict with respect to the proximity constraint, the CVL framework will delete the one with lesser weight, unless deleting the record with higher weight yields are better global solution for that zoom-level. In other words, the user does not control directly how the framework resolves conflicts, but influences the decision by assigning record weights. 

While the user does not need to individually assign weights to records, CVL offers a very flexible scheme for doing this. Weights are assigned by evaluating a SQL expression for each input row. For example a given column in the input database can be used directly as the weight of a record, or the length or area of the record geometry could be used. In fact any floating point expression can be used to weigh records, and it is perfectly ok to use the output of the random number generator or even a constant. This implies that record weight is a partial order, i.e. any number of records can have the same weight.

The only contract between the user and the CVL framework is that the user code must assign a floating point number to each record to use a the weight.


Cartography has a long tradition spanning hundreds of years, and has rightly been considered as much an art as a science. The domain of the data is often considered very important, which is evident in many papers on algorithms for automated cartographic generalization~\cite{areaagg,ordnance,another}. This has changed with the rise and success of social networks and data journalism~\cite{datajournalism}, where the data is often both narrow and massive~\cite{twitter,datablog}. Together with the availability of good quality background maps online~\cite{bing,google,osm} this has opened up the playing field for \emph{domain-agnostic} algorithms~\cite{fusiontables,samet}. For example, a map published by a news agency may display just a single kind of point data from a massive dataset on top of a general purpose background map~\cite{iraq}. The emphasis is not so much on extreme sophistication, but more on the ability to work at large scale and with new datasets appearing all the time.

\subsection{Map services}

A typical architecture used for map services consists of a rendering process and a database process. In this architecture each section of the map is visualized by following these three steps:

\minisec{Scale-oblivious spatial filtering} The rendering process fetches data from the database using a spatial predicate such as \emph{intersects-boundingbox} to fetch data that overlaps the section of the map being rendered. The filtering is computed by the database and is made fast by the use of spatial indexes.

\minisec{Scale-aware attribute filtering} The rendering process applies \emph{post-filtering} to the data that was retrieved, using range and equality predicates according to scale. The predicates come from \emph{cartographic filtering rules} that make statements of what sets of records should be omitted/included at what scales, such as "omit individual buildings on a map of the world" or "include restaurants on a map of a city".

\minisec{Rendering} After the data has been filtered, it is rendered using \emph{cartographic visualization rules} such as "color lakes blue".

Different steps are certainly possible, but these step represent the most typical case. In this work we realize an opportunity for making the rendering process more efficient. 

\marcos{While I can certainly see that WMS@GST uses the process above, it is not clear that the three steps above are an accurate representation of all map services?}
\marcos{It seems to me that it is important to say that rendering is orthogonal to the work, and that the selection task of generalization is computationally intensive and can be done at indexing time. Perhaps these two points can be conveyed with less words than in the section above and the section below? That would leave us space to introduce other important basic definitions listed in the comment at the beginning of this section.}

\subsection{Specializing datasets by fixing filters}
One of the reasons that such map services are hard to scale is that all three steps are performed every time a map visualization is produced. The first filtering step is both I/O and CPU intensive. The second filtering step is mainly CPU intensive. The third visualization step is also mainly CPU intensive. Our goal is to make the overall process less I/O and CPU intensive. Another approach would be to simply make redundancies in the system, by replicating the processes. While the rendering process is fairly cheap to replicate, as it is completely stateless, the database has state (the data) which makes it harder or at least more expensive to replicate.

Here we discuss how a dataset can be specialized to a set of cartographic filtering rules, without fixing the cartographic visualization rules that can be applied later. By doing so the overall task of visualizing the data becomes less I/O and CPU intensive, and the total cost of the system can therefore be expected to decrease. The idea is to do most of the filtering as preprocessing, and deferring only the rendering of the data.

The first realization is that the scale-oblivious spatial filtering does not dependent on how the data will be rendered in the end. A street either intersects a geographical area corresponding to a section of the map or it doesn't. It makes no difference whether the street is eventually colored red or blue. This implies that scale-oblivious spatial filtering can be done once, as pre-processing.

The second realization is that there is a relationship between the principle of constant information density and the scale-aware attribute filtering. If residential streets are excluded from one world map, simply because there are too many, they are likely to be excluded from any other world map. This implies that scale-aware attribute filtering can be done once, as pre-processing.

The task that remains corresponds roughly to deciding which color to draw the records that survived the filtering process, which is outside the focus of CVL.


