% !TEX root = ./cvl.tex
\section{Introduction}

%\marcos{The introduction now reads a bit too generic, and a bit too rich in buzzwords (big data, crowd sourcing). What is the problem that is being addressed?}
%what's the situation

%- why do people need to do cartographic generalization?

%- how do people go about this task today?

%- what is the main related work, and why is it not enough?

\martin{All citations in this section need to be checked - some should perhaps be moved to the related work section (Kostas?)}

Cartographic generalization has a long tradition spanning hundreds of years, and has rightly been considered as much an art as a science~\cite{rieger1993consensus}. In \emph{automated} cartographic generalization the selection, simplification and enhancement of data for representation on a map is performed by algorithms on a computer. Automated cartographic generalization has been quite successful for the construction of general purpose maps, although generally requiring interaction with domain experts in order to design algorithms adjusted to the data domain~\cite{cecconi2003integration,schmid2008automated,regnauld2006improving}. Recent years has seen an abundance of maps produced by crowdsourcing projects~\cite{openstreetmap}, and the combined result is that there is currently a large number high-quality general purpose background maps to choose from online~\cite{}.
%As a consequence of this, we consider the application of automated cartographic generalization to general background maps to be no longer relevant, at least for a very large class of important use cases of maps.

With the rise of social networks, factivism and data journalism~\cite{sankaranarayanan2009twitterstand,bono,cohen2011journalism}, there is a need to handle the generalization of narrow and massive~\cite{twitter,datablog} geospatial datasets. This has opened up the playing field for \emph{domain-agnostic} algorithms for cartographic generalization~\cite{sarma2012fusiontables,nutanong2012multiresolution}. For example, a map published by a news agency may display just a single kind of point data from a massive dataset on top of a general purpose background map. The emphasis is not so much on extreme sophistication, but more on the ability to work at large scale and with new datasets appearing all the time  --- shifting the relevance of automated cartographic generalization away from general purpose background maps toward other kinds of data and maps such as narrower data overlay.

One of the major challenges for automated cartographic generalization for massive datasets is the selection of data for each scale of the generated map; socalled \emph{cartographic filtering rules} make statements of what sets of data should be omitted/included at what scales, such as "omit individual buildings on a map of the world" or "include restaurants on a map of a city". However, specifying explicit filtering rules is not practically feasible for unstructured, massive (and often dynamic) datasets.
 
%what is the approach, why is it cool

%- declarative. extensible. database integrated.

In many current map services, the filtering of data for a given query --- which specifies a scale and a bounding box --- takes place \emph{after} retrieving all the data within the given bounding box. Also, the filtering rules are, as noted above, typically static. This is both I/O and CPU intensive, and not flexible from a users point of view.

In this paper, we present a \emph{database integrated} approach that reduces I/O and CPU load, while at the same time being more flexible. By using a \emph{declarative language}, we allow the user to express the importance of the each data element plus additional constraints. This input is used by an algorithm that performs the actual filtering for each scale in a pre-processing phase. All operations are performed within the database, and the result of the filtering stored for fast execution of subsequent queries.

%four contributions

%- language, express real world constraints very concisely.

%- underlying math mapping makes constraints fully pluggable and reuse existing algorithms from literature.

%- implementation fully executes CVL inside the database.

%- results are usually listed as an explicit contribution (only summarize main points: variety of real datasets, time, quality).

%\marcos{Take into account the questions and flow above when rewriting the introduction}

In this paper, we make the following four contributions:
\begin{enumerate}
\item Define a declarative language, Cartographic Visualization Language (CVL), for generalizing spatial datasets. CVL is designed to be simple and efficient to use for non-cartographers. While fully automatic generalization methods have been demonstrated to work on these types of data, they work best for simple point datasets~\cite{thinningpaper}. With CVL we take a small step back towards giving humans control of the generalization process. Just enough to improve the quality over fully automatic methods, but not so much as to require humans to waste a lot of time managing every detail of the generalization process~\cite{fme}. 

\item Map the associated cartographic filtering problem to the \emph{set multicover problem}, which makes constraints fully pluggable and allows reuse of well-known algorithms.

\item Show how to fully implement CVL inside the database; this enables us to reuse basic database technology for data management and scalability. Thus we move code to data~\cite{mapreduce} instead of vice versa~\cite{fusiontables}. Furthermore, CVL is designed to compile to other languages that can run where the data is stored, e.g. SQL or MapReduce.

\item Present experimental results for a variety of real datasets. The results show that the proposed approach has good performance and produces high-quality carteographic generalizations. [Mention GST somewhere here?] 
\end{enumerate}

In Section~\ref{sec:background} we define the multi-scale filtering problem for cartographic generalization. In Section~\ref{sec:cvl-language} we introduce the CVL language. In Section~\ref{sec:optimizationmodel} we introduce the mapping to the set multicover problem, and we revisit algorithms for this problem in Section~\ref{sec:algorithms}. In Section~\ref{sec:implementation} we introduce the compilation procedure, which enables us to run CVL on a relational database backend. Experimental results are presented in Section~\ref{sec:experimental}, and finally related work is summarized in Section~\ref{sec:related}.
