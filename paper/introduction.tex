% !TEX root = ./cvl.tex
\section{Introduction}

%\marcos{The introduction now reads a bit too generic, and a bit too rich in buzzwords (big data, crowd sourcing). What is the problem that is being addressed?}
%what's the situation

%- why do people need to do cartographic generalization?

%- how do people go about this task today?

%- what is the main related work, and why is it not enough?

%Map generalization has a long tradition spanning hundreds of years, and has rightly been considered as much an art as a science~\cite{rieger1993consensus}. 
\kostas{Can still reduce the amount of nomenclature at use strictly the term multi-scale filtering problem}

\marcos{Not very clear what map generalization really means, now that the examples are completely gone.}
\marcos{GST?}

The goal of map generalization is to produce a map at a given scale, that achieves the right balance between precision and legibility. Achieving this goal requires both data reduction and appropriate graphical symbolization~\cite{brassel1988generalization,gruenreich1985cag}. A benefit of data reduction in digital maps is that performance can improve vastly be reducing the amount of data that needs to be handled when rendering a portion of the map. In \emph{automated} map generalization the generalization process is performed either entirely or partially by algorithms on a computer.

Fully automatic generalization of digital maps~\cite{nutanong2012multiresolution,sarma2012fusiontables} is relevant in many areas such as social networks, factivism and data journalism~\cite{cohen2011journalism,bono,sankaranarayanan2009twitterstand} were there is a constant need for visualizing new and often massive geospatial datasets. Both the legibility and performance benefits of generalization become important as a map gains a large audience. A system for automatic generalization in this context should be able to handle big spatial datasets, consisting of both point and polygon records, should be usable by novice programmers, and be able to finish processing, e.g. in time for a tight news agency deadline. Ideally, such a system will allow users to control the important aspects of solutions using logical and concise measures and reuse existing technology as much as possible.

Spatial data is often stored in a database with powerful spatial extensions installed, so a natural idea is to exploit the processing capabilities of the database to perform map generalization. In this work we present a novel \emph{database integrated} approach which is a complete solution to the data reduction problem in map generalization while deferring graphical symbolization to a later stage~\cite{gaffuri12vectortiles}. All operations are performed entirely within the database process, and the result is a preprocessing of spatial records for fast execution of subsequent scale-parameterized queries~\cite{hilbert1891ueber}. Essentially a number is assigned to each spatial record which is the lowest zoom-level at which the record should be visible in a zoomable map, which allows for efficient indexing.

Using a \emph{declarative language}, we allow the user to concisely express spatial constraints and object importance which used to compute a multi-scale database from an input table of spatial data. Experiments show that the result can be expected to be very close to optimal. This gives users a large amount of control over the map generalization process, while still being extremely concise, as little as four lines of code.

\marcos{Revise arguments wrt. related work + open-source?}

We know of two recent papers which address the problem of data reduction problem~\cite{nutanong2012multiresolution,sarma2012fusiontables}. While both of these approaches provide good solutions to the data reduction problem with good running time, there are distinct and overlapping shortcomings to both of these which are not suffered by our approach.  Both of these approaches support only fixed constraints, while we allow a large class of constraints to be defined by the user. The first paper~\cite{sarma2012fusiontables} seems to indicate that the dataset must fit main memory and implies that data must be serialized in and out of the database for processing, none of which is true of our system. The other published approach~\cite{nutanong2012multiresolution} seems to require modifications to the database engine, which is not true of our system either. Neither of these previously published systems offer a language interface to users, but do imply a mechanism for parameterizing the fixed constraints. While~\cite{sarma2012fusiontables} show that there is at least mathematical support in their approach for several different objective functions, it is not clear how a user would actually express new objectives in a way that is understood by the system. Finally, users can take our implementation and start running it on their own infrastructure using only free, unmodified, open source software.

In this paper, we make the following four contributions:
\begin{enumerate}
\item We present a declarative language, Cartographic Visualization Language (CVL), for generalizing spatial datasets. CVL is designed to be simple and concise to use for non-cartographers while also allowing for efficient and seamless evaluation.

\item We map the data reduction problem in map generalization to the well-known \emph{set multicover problem}~\cite{rajagopalan1998primal}, which makes constraints fully pluggable and allows reuse of well-known algorithms~\cite{rajagopalan1998primal,vazirani2001approximation}.

\item We show how to fully evaluate CVL inside the database; this enables us to reuse basic database technology for data management and scalability. While CVL is designed to compile to a variety of engines~\cite{dean04mapreduce}, we present here an implementation using a relational database engine with spatial extensions.

\item We present experimental results for a variety of real datasets. The results show that the proposed approach has good performance and produces high-quality map generalizations.
\end{enumerate}

In Section~\ref{sec:background} we define the data reduction problem in map generalization as a multi-scale filtering problem. In Section~\ref{sec:cvl:language} we introduce the CVL language. In Section~\ref{sec:optimizationmodel} we formalize the multi-scale filtering problem which is based on a mapping to the set multicover problem, and we revisit algorithms for this problem in Section~\ref{sec:algorithms}. In Section~\ref{sec:implementation} we discuss the compilation procedure that enables us to run CVL on a relational database backend. Experimental results are presented in Section~\ref{sec:experimental}, and finally related work is summarized in Section~\ref{sec:related}.
