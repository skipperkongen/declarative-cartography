% !TEX root = ./cvl.tex
\section{Introduction}

%\marcos{The introduction now reads a bit too generic, and a bit too rich in buzzwords (big data, crowd sourcing). What is the problem that is being addressed?}
%what's the situation

%- why do people need to do cartographic generalization?

%- how do people go about this task today?

%- what is the main related work, and why is it not enough?

Map generalization has a long tradition spanning hundreds of years, and has rightly been considered as much an art as a science~\cite{rieger1993consensus}. The goal of map generalization is to produce a good map at a given scale, which involves both data reduction and choice of graphical symbolization~\cite{brassel1988generalization,gruenreich1985cag}.  In \emph{automated} map generalization this tasks is performed by algorithms on a computer.

In areas such as social networks, factivism and data journalism~\cite{cohen2011journalism,bono,sankaranarayanan2009twitterstand} there is a constant need for visualizing new and often massive geospatial datasets. For example, a news agency may publish a zoomable map that displays interesting spatial data from a social network. A system for this use case must be able to handle big spatial datasets, consisting of both points and polygons, be usable by novice programmers, and finish processing in a time that works for environments with tight deadlines. Ideally, the system will allow users to control the important aspects of solutions and reuse existing technology as much as possible.

Spatial data is often stored in a database with powerful spatial extensions installed, so a natural idea is to exploit the processing capabilities of the database to perform map generalization. In this work we present a novel \emph{database integrated} approach which is a complete solution to the data reduction problem while deferring graphical symbolization to a later stage. All operations are performed entirely within the database process, and the result is a preprocessing of spatial records for fast execution of subsequent scale-parameterized queries. Essentially a number is assigned to each spatial record which is the lowest zoom-level at which the record should be visible in a zoomable map, which allows for efficient indexing.

Using a \emph{declarative language}, we allow the user to concisely express the spatial constraints and an objective function that must be met by a multi-scale database computed from an input table of spatial data. Experiments show that the output can be expected to be very close to optimal. This gives users a large amount of control over the map generalization process, while still being extremely concise, as little as four lines of code.

We know of two recent papers in which solutions to the data reduction problem have been published~\cite{nutanong2012multiresolution,sarma2012fusiontables}. While both of these approaches provide good solutions to the data reduction problem, there are distinct and overlapping shortcomings to both of these which are not suffered by our approach.  Both of these approaches support only fixed constraints, while we allow a large class of constraints to be defined by the user. The first paper~\cite{sarma2012fusiontables} seems to indicate that the dataset must fit main memory and requires data to be serialized in and out of the database for processing, none of which is true of our system. The other published approach~\cite{nutanong2012multiresolution} seems to require modifications to the database engine, which is not true of our system. Neither of these previously published systems offer a language for users, but do provide parameterization of the fixed constraints. While~\cite{sarma2012fusiontables} show that there is at least mathematical support in their approach for several different objective functions, it is not clear how a user would actually express new objectives in a way that is understood by the system. Finally, users can take our implementation and start running it on their own infrastructure using only free, unmodified, open source software.

In this paper, we make the following four contributions:
\begin{enumerate}
\item Define a declarative language, Cartographic Visualization Language (CVL), for generalizing spatial datasets. CVL is designed to be simple and efficient to use for non-cartographers while also allowing for efficient evaluation.

\item Map the generalization problem to the \emph{set multicover problem}, which makes constraints fully pluggable and allows reuse of well-known algorithms.

\item Show how to fully evaluate CVL inside the database; this enables us to reuse basic database technology for data management and scalability. While CVL is designed to compile to a variety of engines, we present here other languages that can run where the data is stored, e.g. SQL or MapReduce.

\item Present experimental results for a variety of real datasets. The results show that the proposed approach has good performance and produces high-quality cartographic generalizations. [Mention GST somewhere here?] 
\end{enumerate}

In Section~\ref{sec:background} we define the multi-scale filtering problem for cartographic generalization. In Section~\ref{sec:cvl:language} we introduce the CVL language. In Section~\ref{sec:optimizationmodel} we introduce the mapping to the set multicover problem, and we revisit algorithms for this problem in Section~\ref{sec:algorithms}. In Section~\ref{sec:implementation} we introduce the compilation procedure, which enables us to run CVL on a relational database backend. Experimental results are presented in Section~\ref{sec:experimental}, and finally related work is summarized in Section~\ref{sec:related}.
