% !TEX root = ./cvl.tex
\section{Introduction}
A problem in data management is generalizing spatial datasets for visualization on zoomable maps. A good map should satisfy two main requirements: it should be visually legible and should be a good representation of the dataset that is visualized. These two conflicting goals should be balanced with respect to the scale and purpose of a given map. In a zoomable map, at dataset is visualized at many different scales, implying that the dataset must be generalized for each of these scales. 

Generalization is inherently a process of reduction. The goal is to reduce visual ``clutter'' which is usually achieved by a reduction of the data. When datasets have millions or even billions of records, the best option is often to filter out a subset of the records, using a measure to prioritize records that should remain~\cite{sarma2012fusiontables}. This is known in the generalization literature as \emph{selection}~\cite{weibel1999generalising}.
%Other options include \emph{displacement} and \emph{aggregation} of data, but these techniques are not always applicable or even meaningful. For instance it is not immediately obvious how to aggregate geocoded images or messages from social media.

Fully automatic generalization of digital maps is increasingly relevant due to the constant rise of geocoded data on the web, coupled with a rising demand to understand and visualize this data. Exciting new areas driving the demand include social media, factivism and data journalism~\cite{cohen2011journalism}. 
%In these areas there is a constant need for visualizing new and often massive geospatial datasets.

To meet the demand, a map generalization framework should be able to handle big spatial datasets and be usable by people with little or no programming experience. This implies the need to express the generalization task in concise terms understood by novice users. It also implies the need for an implementation that, if need be, can be scaled effortlessly.

Unfortunately, current approaches to map generalization fall short in one or both of these aspects. Systems have been developed which apparently work fully automatically, but typically these are confined to the memory of a single machine (are not scalable) or do not allow users to control the generalization process (are not programmable)~\cite{sarma2012fusiontables}. 


above dimensions. Recent work has mostly considered only explicit rules or pre-set constraints for map generalization, resulting in solutions that are either too complex~\cite{sld,mapnik}, or too restrictive for novice programmers~\cite{sarma2012fusiontables,nutanong2012multiresolution}. In addition, previous solutions have been poorly integrated with existing technology, resulting in scalability bottlenecks such as being restricted to the main memory capacity of a single node~\cite{sarma2012fusiontables}. 

Spatial data is often stored in a database with powerful spatial extensions installed, so a natural idea is to exploit the processing capabilities of the database to perform map generalization. In this work, we present a novel \emph{database integrated} approach that is a complete solution to the data reduction problem in map generalization. All operations are performed entirely within the database process, and the result is a preprocessing of spatial records for fast execution of subsequent scale-parameterized queries~\cite{hilbert1891ueber}. Essentially a number is assigned to each spatial record corresponding to the lowest zoom-level at which the record should be visible in a zoomable map, allowing for efficient indexing.

Using a \emph{declarative language}, we allow the user to concisely express spatial constraints and object importance, which are used to compute a multi-scale database from an input table of spatial data. This gives users a large amount of control over the map generalization process, while still being extremely concise, expressing a generalization with as little as four lines of code.

%\marcos{Revise arguments wrt. related work + open-source?}
%\martin{The paragraph below belongs, I think, to the related work section. Alternatively, it can be shortened a bit here and the extended version given in the related work section.}
%
%We know of two recent papers which address the problem of data reduction problem~\cite{nutanong2012multiresolution,sarma2012fusiontables}. While both of these approaches provide good solutions to the data reduction problem with good running time, there are distinct and overlapping shortcomings to both of these which are not suffered by our approach.  Both of these approaches support only fixed constraints, while we allow a large class of constraints to be defined by the user. The first paper~\cite{sarma2012fusiontables} seems to indicate that the dataset must fit main memory and implies that data must be serialized in and out of the database for processing, none of which is true of our system. The other published approach~\cite{nutanong2012multiresolution} seems to require modifications to the database engine, which is not true of our system either. Neither of these previously published systems offer a language interface to users, but do imply a mechanism for parameterizing the fixed constraints. While~\cite{sarma2012fusiontables} show that there is at least mathematical support in their approach for several different objective functions, it is not clear how a user would actually express new objectives in a way that is understood by the system. Finally, users can take our implementation and start running it on their own infrastructure using only free, unmodified, open source software.

\vspace{5em}

In this paper, we make the following four contributions:
\begin{enumerate}
\item We present a declarative language, Cartographic Visualization Language (CVL, pronounced ``civil''), for generalizing spatial datasets. CVL is designed to be simple and concise to use for novice programmers. The CVL language was designed in collaboration with the Geodata Agency~\footnote{\texttt{http://www.gst.dk/English/}} and Grontmij Engineering~\footnote{\texttt{http://grontmij.dk/}} in Denmark.

\item We map the data reduction problem in map generalization to the well-known \emph{set multicover problem}~\cite{rajagopalan1998primal}, which makes constraints fully pluggable and allows reuse of well-known algorithms~\cite{rajagopalan1998primal,vazirani2001approximation}.

\item We show how to fully evaluate CVL inside the database; this enables us to reuse basic database technology for data management and scalability. While CVL is designed to compile to a variety of engines~\cite{Stonebraker:2010:PDBMSvsMapReduce}, we present here an implementation using a relational database engine with spatial extensions.

\item We present experimental results for a variety of real datasets. The results show that the proposed approach has good performance and produces high-quality map generalizations.
\end{enumerate}

In Section~\ref{sec:background}, we define the data reduction problem in map generalization as a multi-scale filtering problem. In Section~\ref{sec:cvl:language}, we introduce the CVL language. In Section~\ref{sec:optimizationmodel}, we formalize the multi-scale filtering problem which is based on a mapping to the set multicover problem, and we revisit algorithms for this problem in Section~\ref{sec:algorithms}. In Section~\ref{sec:implementation}, we discuss the compilation procedure that enables us to run CVL on a relational database backend. Experimental results are presented in Section~\ref{sec:experimental}, and finally related work is summarized in Section~\ref{sec:related}.
