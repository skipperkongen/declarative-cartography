% !TEX root = ./cvl.tex
\section{Introduction}
A problem in data management is generalizing spatial datasets for visualization on zoomable maps. A good map should satisfy two main requirements: it should be visually legible and should be a good representation of the dataset that is visualized. These two conflicting goals should be balanced with respect to the scale and purpose of a given map. In a zoomable map, at dataset is visualized at many different scales, implying that the dataset must be generalized for each of these scales. 

Generalization is inherently a process of reduction. The goal is to reduce visual ``clutter'' which is usually achieved by some kind of reduction of the data. When datasets contain millions or even billions of records, the best option is often to filter out a subset of the records, using a measure to prioritize records that should remain~\cite{sarma2012fusiontables}. Generalization by omitting records is known in the generalization literature as \emph{selection}~\cite{weibel1999generalising}.
%Other options include \emph{displacement} and \emph{aggregation} of data, but these techniques are not always applicable or even meaningful. For instance it is not immediately obvious how to aggregate geocoded images or messages from social media.

Fully automatic generalization of digital maps is increasingly relevant due to the constant rise of geocoded data on the web, coupled with a rising demand to understand and visualize this data. Exciting new areas driving the demand include social media, factivism and data journalism~\cite{cohen2011journalism}. 
%In these areas there is a constant need for visualizing new and often massive geospatial datasets.

To meet the demand, a map generalization framework should be able to handle big spatial datasets and be usable by people with little or no programming experience. This implies the need to express the generalization task in concise terms understood by novice users. It also implies the need for an implementation that, if need be, can be scaled effortlessly.

Unfortunately, current approaches to map generalization fall short in one or both of these respects. Systems have been developed which apparently work fully automatically, but typically these are confined to the memory of a single machine (are not scalable) or the generalization process is hard or even impossible to control for novice users (are not programmable)~\cite{sarma2012fusiontables}. We address these shortcomings by several innovations, which are combined in our solution.

To address the programmability issue, we have designed a declarative language, CVL, which is arguably simple enough to be used by novice programmers. The core idea of this language is that users should express generalization in terms that are easily understood; meaning constraints that must be satisfied by the solution and how the importance of records should be measured. Users don't have to express the parts which are hard to grasp, such as the algorithms used to compute the results. This is essentially a concise, declarative program.

To address the scalability issue, the declarative program is then mapped by the framework on two levels. On the algorithmic level, the program is mapped to a well-known optimization problem, for which good algorithms exist. Spatial data is often stored in a database with powerful spatial extensions installed, so a natural idea is to exploit the processing capabilities of the database to perform map generalization. On the language level, the declarative program written in CVL, is compiled to a database program expressed in SQL. This allows the program to be executed on highly scalable databases, which could theoretically include parallel databases. The benefits of moving \emph{code-to-data} instead of vice-versa include: better utilization of network bandwidth, exploiting locality and using existing indexing of the spatial data~\cite{Guttman1984:RTree,Hellerstein1995:GiST}. We call our approach \emph{Declarative Cartography}.

\vspace{5em}

In summary, we make the following four contributions in this paper:
\begin{enumerate}
\item We present a declarative language, Cartographic Visualization Language (CVL, pronounced ``civil''), for generalizing spatial datasets. CVL is designed to be simple and concise to use for novice programmers. The CVL language was designed in collaboration with the Danish Geodata Agency~\footnote{\texttt{http://www.gst.dk/English/}} and employees at the engineering company Grontmij~\footnote{\texttt{http://grontmij.dk/}} in Denmark.

\item We map the data reduction problem in map generalization to the well-known \emph{set multicover problem}~\cite{rajagopalan1998primal}, which makes constraints fully pluggable and allows reuse of well-known algorithms~\cite{rajagopalan1998primal,vazirani2001approximation}.

\item We show how to fully evaluate CVL inside the database; this enables us to reuse basic database technology for data management and scalability. While CVL is designed to compile to a variety of engines~\cite{Stonebraker:2010:PDBMSvsMapReduce}, we present here an implementation using a relational database engine with spatial extensions.

\item We present experimental results for a variety of real datasets. The results show that the proposed approach has good performance and produces high-quality map generalizations.
\end{enumerate}

In Section~\ref{sec:background}, we define the data reduction problem in map generalization as a multi-scale filtering problem. In Section~\ref{sec:cvl:language}, we introduce the CVL language. In Section~\ref{sec:optimizationmodel}, we formalize the multi-scale filtering problem which is based on a mapping to the set multicover problem, and we revisit algorithms for this problem in Section~\ref{sec:algorithms}. In Section~\ref{sec:implementation}, we discuss the compilation procedure that enables us to run CVL on a relational database backend. Experimental results are presented in Section~\ref{sec:experimental}, and finally related work is summarized in Section~\ref{sec:related}.
