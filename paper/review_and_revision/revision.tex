\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}

\title{Working document for revision of ICDE '14 paper}
\author{Kostas, Martin, Marcos}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle

\section{About}
Notes and overall plan recorded in Kostas/Marcos Skype chat October 16 2013. Citations for ladder/star approach in Kostas/Pia chat November 12 2013. Citations for NP-hardness in Kostas/Martin email.

\section{Renamings to be consistent with literature}

\begin{itemize}
\item Rename ``filtering problem'' to ``thinning problem'' or ``selection problem''; consistent with the \emph{selection operator} in generalization literature, or \emph{thinning} in the fusion tables paper.
\item Rename ``bottom-up'' to ``ladder'' approach; consistent with name for this approach in the generalization literature~\cite{foerster2010challenges}
\end{itemize}

\section{Why map conflict resolution to an NP-hard problem?}

\begin{description}
\item[Hardness proof (tags: algorithm section)] If we model the problem using conflict sets, the problem is
definitely NP-hard for all interesting cases.

For an informal proof, consider the vertex cover problem. Given a graph G=(V,E),
find a minimum size subset of the vertices S such that every edge in E
has an endpoint in S. Think of the vertices as records and the edges
as conflict sets. This problem would then model the filtering problem
for the special case where all conflict sets had exactly two records
(and all records had identical weights).

The vertex cover problem is NP-hard, even for very constrained cases.
For example, even if G is a planar graph and every vertex has degree
at most 3, the problem remains NP-hard~\cite{garey1977rectilinear}. In other words, even if the
conflict sets only have two records each, and each record is involved
in at most 3 conflict sets, the problem remains NP-hard. It is hard to
imagine that any interesting application is more restrictive.

Note that the cubic vertex cover problem is APX-hard - which
means that it cannot be approximated arbitrarily close unless P=NP~\cite{alimonti2000some}. So even this restricted version is ``really'' hard.
\end{description}



\section{Why use conflict sets to model problem?}


\section{Refinements based on knowledge gained in Z\"{u}rich}

\begin{description}
\item[Requirements of a generalized map] There are two main requirements that a generalized map must satisfy. Firstly, it must be a correct, though abstracted, representation of the physical reality. Secondly, the map must be readable by the user. If both these two requirements are not taken into account the map will not be of high quality. The final generalized map will eventually be a compromise between good readability and good representation~\cite{harrie2007modelling}.

\item[Modeling the generalization process (tags: method, related work)]  (From ``Modelling the overall process of generalisation''~\cite{harrie2007modelling} by Harrie and Weibel) 

A generalized map should satisfy several \emph{conditions}. The main idea behind \emph{constraint based modeling} is to let these conditions act as \emph{constraints} in the generalization process. The advantage of using constraints instead is that they are not bound to a certain action~\cite{beard1991constraints}. Using constraint based modeling thus leaves flexibility in defining the method by which a satisfaction of the constraints is sought.

To use constraints in map generalization they must have a \emph{measure}. In order for the generalized map to satisfy all constraints all the measures must have satisfying values, e.g. the distance between objects must be greater than some minimum threshold. 

Most often, however, there exists no generalization solution that completely satisfies the constraints and the solution must be a compromise. To find the best compromise there must be a \emph{cost} connected to the violations of the constraints (computed by the measure). The optimal generalized map is then the solution with the lowest total cost. Several methods for finding the optimal solution have been described in the literature, including \emph{agent modelling}, \emph{combinatorial optimization} and \emph{continuous optimization}~\cite{harrie2007modelling}.



Note that one should be careful with the use of the word \emph{optimal} here. That a generalized map is an optimal solution, in a constraint based modeling sense, does not mean anything more than that the constraints used are violated as little as possible. This implies that a solution will never be better than the constraints used. To compute a perfect generalized map would in addition require that we have a complete set of constraints for the generalized map. 

Currently, the constraints used for generalization are limited in the sense that they cannot describe all aspects of a generalized map; hence, the generalized map using constraint based modeling is not optimal in a more \emph{general} sense.

\item[Expressibility of constraints (tags: limitations, related work)] Various typologies of constraints have been discussed in the literature~\cite{beard1991constraints,harrie2007modelling}. In the typology of Harrie and Weibel~\cite{harrie2007modelling}, we consider only \emph{legibility constraints} such as proximity and density constraints, leaving aside \emph{appearance constraints} that serve the purpose of maintaining faithful representation of the input data. 

Future work should allow appearance constraints~\cite{harrie2007modelling} to be formulated, such as maintaining the \emph{spatial distribution} of a dataset and maintaining \emph{topological relationships} such as \emph{adjacency}, \emph{connectivity}, \emph{accessibility} and \emph{containment}.

Our current approach is to use an \emph{objective function} to balance the legibility constraints, such that the combined weight of objects that are removed is minimized. 

(Note: Stephan Schmid wrote a nice master's thesis that includes a thorough discussion of constraints~\cite{schmid2008automated}. Ask Weibel about what grade it got).

\item[Ladder/star approach (tags: method, limitations, related work)] When generalizing a dataset for several scales, an important distinction is whether to follow the \emph{ladder} or \emph{star} approach~\cite{foerster2010challenges}. We have only implemented the ladder approach, where lower scales are recursively derived from higher scales. An advantage of using the ladder approach is that the zoom-consistency constraint is automatically satisfied. The star approach is well fitted for use cases where enforcing the zoom-consistency constraint is not appropriate, e.g. when generalizing regional labels. Often regional labels appear on intermediate scales, but not at lower or higher scales.

\item[Topology (tags: limitations)] Our method does not yet support topological constraints to be formulated. The aim of topological constraints is to maintain correct topological relationships such as . Allowing topological constraints to be expressed is an important direction for future work.

\end{description}

\section{What we promised the reviewers}

\begin{description}
\item[Changing geometry (tags: discussion)] (copied from author-feedback) The question regarding changing the geometry type is very relevant and we have discussed it extensively while developing our framework. Let us first consider the semantics of changing geometry type. We can have a set-based semantics, in which a set of objects with given geometries are changed into a different set of objects (e.g., multiple objects are aggregated into one). Alternatively, we can consider a per-object semantics, in which individual objects remain identifiable throughout the generalization process, but their geometries change either by simplification (reducing the fidelity) or collapse (converting, e.g., a polygon to a point). Set-based semantics would imply changing dynamically the very formulation of the multi-scale filtering optimization problem. Given this complexity, we favor the per-object semantics, assuming that geometry changes are not significant enough to affect the solution. Under this assumption, a per-object semantics would be easy to incorporate in CVL through an extension to the generalize statement specifying a geometry modification function.

From an implementation point of view, such an extension can be supported in our algorithmic framework either as a post-processing step to the geometries before finalization, or as a geometry transformation step per zoom level. The latter alternative offers the more interesting opportunity of having multiple object representations on different zoom-levels, necessitating object versioning during generalization. Under the assumption above that geometry changes do not affect the optimization problem, this extension could actually provide us with potential performance gains, since it is faster to compute predicates over simple objects than complex ones. We plan to include in our paper a summarized discussion of this important issue of multiple object representations.

\item[Running time (tags: discussion)] (Copied from auther-feedback) It is an important observation, with which we agree, that the running time of the LP-based greedy algorithm is high. We implemented this algorithm as an alternative to the SQL-based SGA-algorithm mainly because it provides a bound on the solution quality. It should be noted that we have investigated other algorithms which could potentially have much better performance, albeit providing looser bounds on quality (ref [34] in the paper). These algorithms allow for different trade-offs between quality and performance, as well as careful reasoning about SQL-based implementations, to be explored. This is an interesting avenue for future work. We plan to add a discussion on this possibility to the paper, as well as address the other comments on your review.
\end{description}

\section{Addressing selected comments in review}

\begin{itemize}
\item R3 says:  Section II: it seems you do not handle "regional labels" that might reappear at more general zoom levels. (Eg, ``Europe'' reappears when the camera pulls out far enough.) That should be made explicit in the text.
\item R3 says: Not sure why the ``AT X ZOOM LEVELS'' clause is useful. Can you provide a motivating example? Current examples don't make the case. E.g., why ``18'' in Figure 4?
\end{itemize}


\section{TAIL}

We have conflict sets because we want to satisfy constraints. We want to satisfy constraints because it is a common way to reason about feasible solutions to generalization... thinning and "The constraint method for solving spatial conflicts in cartographic generalization"


\section{Question: Why map conflict resolution to an NP-hard problem?}
Here we assume that using conflict sets is a good idea. The question whether that is true is a separate question, that we will also answer.
\begin{itemize}
\item Definition of conflict resolution: Given a set of conflict sets $C$, and a number $\lambda_c < \left\vert c \right\vert$ for each set $c \in C$, choose $\lambda_c$ elements from each set $c$ that will be ``deleted'' 
\item Global optimization version: choose $\lambda_c$ elements of minimum total weight from each set $c$ that will be ``deleted'' 
\item This optimization problem equals set multicover problem, plain and simple. I don't see that we are even ``mapping'' to this problem. It is the same problem?
\item Is there another optimization problem (not minimizing total weight of elements deleted) that involves conflict sets and generates useful solutions to generalization?
\item The SGA algorithm is an exact algorithm for another optimization problem (minimize for each set); but is that really the ``right'' problem to solve and why not?
\end{itemize}

\subsection{Our answer}

\begin{itemize}
\item Actually, for most instances the bulk of the time is spent finding conflicts, not solving the set multicover problem, because in the end we don't solve SMCP
\item We apply approximation algorithms and heuristics which are exact solutions to various other problems that somehow relate to SMCP
\end{itemize}

\section{Question: Why use conflict sets to model problem?}

\begin{itemize}
\item Is there a "conflict free" formulation that would be just as ``good'' and not lead to solving an NP-hard problem?
\item Conflict sets allow a generic way to express many natural user-defined constraints (proximity, visibility)
\item Using conflict sets is a natural way to think about generalization
\item Maybe there is another model that could express proximity and visibility constraints, and which is a natural way to think about generalization, and which leads to an optimization problem that has a polynomial time algorithm?
\end{itemize}

\subsection{Our answer}

\begin{enumerate}
\item We chose conflict sets because it is a natural way to think about generalization and because it allows users to formulate several natural spatial constraints (proximity, visibility etc)
\item We can not think of another model with these properties and which also leads to an optimization problem that has an efficient solution
\item Asking us to prove that such a model does not exist is a tall order...
\end{enumerate}

\bibliographystyle{abbrv}
\bibliography{revision}  % gvl.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references

\end{document}  